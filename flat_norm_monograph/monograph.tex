\documentclass[]{article}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[mathscr]{euscript}
\usepackage{hyperref}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{example}{Example}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{remark}{Remark}[section]
\usepackage{graphicx}

\newcommand{\restrict}{%
  \,\raisebox{-.127ex}{\reflectbox{\rotatebox[origin=br]{-90}{$\lnot$}}}\,%
}

%opening
\title{Flat Norm Monograph}
\author{Curtis Michels \and Kevin Vixie}

\begin{document}

\maketitle

\begin{abstract}
An intuitive explanation of GMT and the Flat Norm.
\end{abstract}

\tableofcontents

\newpage

\section{Introduction}

(Kevin)

\section{Applied GMT}

(Curtis)

\subsection{Measure Theory}

Mathematics is (at least in part) an exploration of spaces. We look at interesting structures and try to classify them to the best of our ability (when are two instances the same, when are they different, etc.) a more precise definition of what we mean when we say ``space" in a mathematical sense follows:

\begin{definition}[Space]
A space is a set possibly equipped with additional structure, often written as a tuple. The simplest example of a space is merely a set by itself.
\end{definition}

\begin{example}
A metric space $X = (\Omega,d)$ is a set $\Omega$ (whose elements are often called points) together with a distance function $d:\Omega^2 \to \mathbb{R}$ that tells us the distance between any two points in $\Omega$. To recover our intuition surrounding distance, we require $d$ satisfy the following for all points $x,y,z \in \Omega$:

\begin{enumerate}
\item The distance from a point to itself is $0$

$d(x,x) = 0$
\item The distance between two distinct points is always positive

if $x \neq y$ then $d(x,y) > 0$
\item The distance from $x$ to $y$ is always the same as the distance from $y$ to $x$:

$d(x,y) = d(y,x)$
\item The distance to go directly from $x$ to $y$ is less than or equal to the distance from $x$ to an intermediate point $z$ plus then the rest of the distance $z$ to $y$.

$d(x,y) \leq d(x,z) + d(z,y)$
\end{enumerate}
\end{example}

\begin{remark}
Condition 2 is often referred to as ``positvity", condition 3 is ``symmetry" and condition 4 is called the ``triangle inequality"
\end{remark}

\begin{remark}
Even if a space $X$ has additional structure, when we speak of ``the elements of a space $X$" we are referring to the elements of the set $\Omega$ which underlies the space.
\end{remark}

Suppose we have a space $X$, and we would like some way to assign notions of length, area, or volume to the subsets of $X$. Denote the set of all subsets of $X$ by $2^X$. We would like to then develop a theory that yields useful theorems for a function $\mu:2^X \to [0,\infty]$ which takes in a subset of $X$ and assigns it to a nonnegative extended real number (possibly infinity). 

\begin{example}
Let $X = (\mathbb{R}^2,d)$ be the two dimensional Euclidean metric space where for two points $x = (x_1,x_2)$, and $y = (y_1,y_2) \in \mathbb{R}^2$ we have the Euclidean metric $d(x,y) = \sqrt{(x_1-y_1)^2+(x_2-y_2)^2}$. Think about how we might assign areas to possible subsets of $X$, like the open balls of radius $r$ centered at the origin $B(0,r) := \{x = (x_1,x_2) : x_1^2+x_2^2 < r^2\}$ and other geometric shapes, without needing to specify them all by hand. i.e. we want a function that will tell us the area of \textbf{any} set. This is the problem we are trying to solve.
\end{example}

The right place to look for such functions is measure theory, in particular the notion of a so-called \textbf{outer measure} will be useful to us. 

\begin{definition}[Outer Measure]
Let $X$ be any space. A function $\mu:2^X \to [0,\infty]$ is called a ``outer measure on $X$" if:

\begin{enumerate}
\item The measure of the empty set is 0

$\mu(\emptyset) = 0$
\item If $A$ is a subset of $B$ then the measure of $A$ is less than or equal to the measure of $B$

$A \subseteq B \implies \mu(A) \leq \mu(B)$
\item If we have a sequence of subsets of $X$ then the measure of their union is less than or equal to the sum of their measures

$A_1,A_2, \ldots \subseteq X \implies \mu\left(\bigcup\limits_{j=1}^\infty A_j \right) \leq \sum\limits_{j=1}^\infty \mu (A_j)$
\end{enumerate}
\end{definition}

\begin{remark}
Condition 2 is sometimes referred to as ``monotone" and condition 3 ``subadditive"
\end{remark}

\begin{remark}
Note that for some measures and choice of sets condition 3 may say nothing more than $\infty \leq \infty$. No special care with regards to convergence is required to talk about the sum, since we are adding non-negative numbers up. It will either be finite or $\infty$, there is no oscillation to worry about. 
\end{remark}

In working with the above definition for some time, we may recognize that there are certain subsets of a given space that are ``nicer" in a sense to work with, that obey a deeper intuition. The following definition is due to Carath\'eodory who developed a way to identify these so-called measurable sets.

\begin{definition}
Let $\mu$ be a outer measure on $X$. Then a subset $A \subseteq X$ is ``$\mu$-measurable if it reasonably ``cuts" every other set. To be precise, for every set $B \subseteq X$ we must have:

\begin{align*}
\mu(B) = \mu(B \setminus A) + \mu(B \cap A)
\end{align*}
\end{definition}

The above says that a set $A$ is measurable if for every set $B$, the measure of the piece of $A$ in $B$ plus the measure of the chunk of $B$ without $A$ recovers the entire measure of $B$. You may think this definition a bit strange, and it may be difficult to think of measures on spaces and sets within them that do not have this property. And indeed, even the proof that sets that are not measurable exist in simple spaces (like the real number line equipped with standard calculus tools) is challenging and difficult. However, such non-measurable sets exist there, and furthermore are fundamental in a sense, as it may be proven that every reasonable measure of length on said number line yields sets that are not measurable. 

It has yet to be seen why this is a good definition, but soon enough this will become clear. 

\begin{remark}
For any space $X$ and outer measure $\mu$ on $X$, it is easy to check from the definition above:
\begin{enumerate}
\item The empty set and the entire space $\emptyset$, $X$ are always $\mu$-measurable.
\item All null sets (sets for which $\mu(A) = 0$) are $\mu$-measurable.
\item If $A$ is $\mu$-measurable then $X \setminus A$ is also $\mu$-measurable.
\end{enumerate}
\end{remark}

\begin{definition}
A family (set of sets) $\mathscr{A}$ of subsets of a space $X$ is called a $\sigma$-algebra if:

\begin{enumerate}
\item $\emptyset, X \in \mathscr{A}$
\item If $A \in \mathscr{A}$ then so to is its relative complement $X \setminus A \in \mathscr{A}$
\item Any sequence $A_1, A_2, \ldots \in \mathscr{A}$ is closed in $\mathscr{A}$ under unions $\bigcup\limits_{j=1}^\infty A_j \in \mathscr{A}$
\end{enumerate}
\end{definition}

\begin{remark}
Although it is not part of the definition, the 3 conditions specified imply that if $A_1, A_2, \ldots \in \mathscr{A}$ then so too is their intersection $\bigcap\limits_{j = 1}^\infty A_j \in \mathscr{A}$. This follows from De Morgan's laws regarding unions and compliments. 
\end{remark}

From the definitions, it is easily checked that given a measure $\mu$ on $X$ the collection of all $\mu$-measurable sets form a $\sigma$-algebra.

\begin{remark}
In probability and statistics, very often it is common to instead start with a $\sigma$-algebra $M$ on a space $X$ and then given a measure $\mu$, to refer to all sets in $M$ as being measurable.
\end{remark}

\begin{theorem}[Existence of a smallest $\sigma$-algebra]
Let $A$ be a (possibly uncountable) collection of subsets in $X$. Then there exists a ``smallest" sigma algebra $\mathscr{A}$ that contains all of the sets in $A$. 

\begin{proof}
Consider the set of all $\sigma$-algebras that contain the subets of $A$, indexed by some possibly uncountable set $I$, denoted $\{\mathscr{A}_\alpha\}_{\alpha \in I}$. 

Then it is easily checked by definition that the intersection of all these $\sigma$-algebras (which is obviously the smallest), is itself a $\sigma$-algebra.

\begin{align*}
\mathscr{A} = \bigcap_{\alpha \in I} \mathscr{A}_\alpha
\end{align*}
\end{proof}
\end{theorem}

Hence given any collection of subsets for a given space, we can form a $\sigma$-algebra. There is a very important special case in which we do this all the time:

\begin{definition}
Let $(X,\mathcal{O})$ be a topological space. Then the smallest $\sigma$-algebra containing the sets in $\mathcal{O}$ (often called the open sets in $X$) is called the Borel $\sigma$-algebra.
\end{definition}

We will now attempt to convince ourselves that Carath\'eodory's definition of a measurable set is a good one in the sense that it leads to powerful measures with lots of measurable sets in a general setting.

\begin{definition}
Let $X$ be a topological space. A measure $\mu$ on $X$ is Borel regular if:

\begin{enumerate}
\item All Borel sets are $\mu$-measurable. (All of the sets in the Borel $\sigma$-algebra)
\item For every set $A \subseteq X$ there exists a borel set $A \subseteq B$ with $\mu(B) = \mu(A)$
\end{enumerate}
\end{definition}

\begin{remark}
It is a common mistake to assume that because $\mu(B) = \mu(A)$ that $\mu(B \setminus A) = 0$. But this is only true under certain circumstances. Suppose $A$ is measurable, that implies $\mu(B) = \mu(B \setminus A) + \mu(B \cap A)$ by the definition of measurability. Since $A \subseteq B$ we have that $B \cap A = A$ and thus $\mu(B) = \mu(B \setminus A) + \mu(A)$, one may be tempted to conclude from this that $\mu(B \setminus A) = 0$, but because we allow measures to be infinite, we cannot simply subtract $\mu(A)$ from both sides lest we get $\infty - \infty$ which is left undefined. Thus we must assume that $A$ is measurable and $\mu(A) < \infty$ to conclude $\mu(B \setminus A) = 0$.
\end{remark}

\begin{theorem}[Carath\'eodory's Theorem]
Let $X$ be a metric space and $\mu$ be any outer measure on $X$ such that $\mu(A \cup B) = \mu(A) + \mu(B)$ whenever $d(A,B) > 0$. Then
\end{theorem}

\begin{remark}
If $d(a,b)$ is the distance between two points then we define the distance between two sets of points $d(A,B) = \inf \{d(a,b) : a \in A, b \in B\}$ then all Borel sets in the Borel $\sigma$-algebra generated by the open balls under $d$ are $\mu$-measurable.
\end{remark}

The assumption above is satisfied for many reasonable measures, intuitively because if the sets are separated by some distance then they shouldn't have any overlap - thus their measures should simply add.

Sometimes it is useful to measure the amount of one set that lies in another. For this we define a notion of restriction.

\begin{definition}
Let $\mu$ be a measure on $X$ and let $A \subseteq X$. The we define the restriction of $\mu$ to $A$ by:

\begin{align*}
\mu \restrict A (B) = \mu(A \cap B)
\end{align*}
\end{definition}

\begin{remark}
It's easy to see that the above defines a new measure, and that if a set $B$ is $\mu$ measurable then it is also $\mu \restrict A$ measurable.
\end{remark}

\subsection{Integration, Generalized}

We now turn to defining integration with respect to a measure $\mu$. If you recall, the familiar Riemann integral from undergraduate calculus $\int f(x) dx$ is defined using sums that look like $\sum f(x) \Delta x$. Some may even have nightmares approximating such integrals over some interval $(a,b)$ by hand by drawing boxes under curves at points $\{x_i\}_{i=1}^n \subseteq (a,b)$ of some fixed width $\Delta x$. It would be very natural then for us to define our integrals using sums with terms like:

\begin{align*}
f(x_i)\mu((x_i,x_{i+1}))
\end{align*}

But it turns out that we can do better! In the sense that a slight alteration will allow us to integrate everything the above would lead to, and more. We will instead use sums that have terms in the form of:

\begin{align*}
y_i \mu(f^{-1}((y_i,y_{i+1})))
\end{align*}

We still have a height times a length, but now we're measuring the preimage of a chunk of the $y$-axis (which itself is a chunk of the $x$-axis). This small dance turns out to be rather powerful, and with it emphasis is placed on functions that when we preimage open sets, give us sets we know to be measurable (i.e. we can do this trick!) we choose to call these functions \textbf{measurable functions}.

\begin{definition}
Suppose $\mu$ is a measure on $X$. Let $Y$ be a topological space. Let $f:X\to Y$ be a function from $X$ into $Y$. Then $f$ is called a $\mu-$measurable function if and only if for every open set $U \subseteq Y$ we have $f^{-1}(U) \subseteq X$ is a $\mu$-measurable set.
\end{definition}

\begin{definition}
Let $\mu$ be a measure on $X$. Then a function $g:X\to [-\infty,\infty]$ is called a $\mu$-step function (if the context of the measure is clear, simply a ``step function") if $g$ is $\mu$-measurable and the image of $X$ under $g$ (aka the range), $g(X) := \{ g(x) : x \in X\}$ is countable as a set. 
\end{definition}

\begin{remark}
The reason why we care about step functions is because we know that sums $\sum$ require a countable index set to sum over.
\end{remark}

\begin{definition}
A $\mu$-step function is called $\mu$-integrable if the sum:

\begin{align*}
I(g) := \sum_{y \in g(X)} y\mu(g^{-1}(y))
\end{align*}

Converges in $[-\infty,\infty]$. 
\end{definition}

\begin{remark}
In the above definition we will agree that $0 \cdot \infty = 0$, and that $\infty - \infty$ is undefined.
\end{remark}

\begin{definition}
A $\mu$-integrable function is called $\mu$-summable if $I(g) \in (-\infty, \infty)$.
\end{definition}

Now we can define integration properly.

\begin{definition}
Let $f: X \to [-\infty,\infty]$ and let $S_\mu$ be the set of all $\mu$-integrable step functions. Define the upper integral of $f$ with respect to $\mu$ over $X$ to be:

\begin{align*}
\int^* f d\mu := \inf \{I(g) : g \in S_\mu, f \leq g, \mu-\text{a.e.}\}
\end{align*}

Similarly the lower integral:

\begin{align*}
\int_* f d\mu := \sup \{I(g) : g \in S_\mu, g \leq f, \mu-\text{a.e.}\}
\end{align*}
\end{definition}

\begin{definition}
Such a function $f$ as above is $\mu$-integrable if and only if:

\begin{align*}
\int^* f d\mu = \int_* f d\mu
\end{align*}

And we call this value the integral of $f$ with respect to $\mu$ over $X$ and denote it by $\int f d\mu$.
\end{definition}

\subsection{Hausdorff Measures}

Hausdorff measures are some of the monst important measures we have in Geometric Measure Theory and provide not only a reasonable notion of length, area, and volumes but also dimension in a very general way. Their definition will be in two parts.

\begin{definition}
Let $X$ be a metric space. Define the diameter of a set $A$ to be $\text{diam}(A) = \sup\{d(x,y) : x,y \in A\}$. For any set $S \subseteq X$ and any real numbers $k, \delta \geq 0$, define the $k$-dimensional measure of $S$ by sets of diameter $\leq \delta$ by:

\begin{align*}
\mathcal{H}_\delta^k(S) :=  \inf \left\{ \sum_j \frac{\omega_k \cdot \text{diam}(E_j)^k}{2^k} : S \subseteq \bigcup_j E_j, \text{diam}(E_j) \leq \delta \right\}
\end{align*}

Where the infimum is taken over all countable covers $S \supseteq \bigcup\limits_j E_j$ constrained so that each set $E_j$ has a diameter at most $\delta$. $\omega_k$ is the volume of the unit $k$-dimensional ball. 
\end{definition}

The value of our dimensional constant is $\omega_k := \frac{2^k \pi^{k/2}}{\Gamma(\frac{k}{2}+1)}$, where $\Gamma$ denotes the gamma function. This constant is chosen so that our measure agrees with intuitive convention.

The above definition may seem quite complex at first, but it is quite simple and natural. Suppose that you are given some flat shape and you want to measure its area, but all you have are post-it notes of varying sizes. One thing you can do is to try to cover the object with the post it notes and then add up the areas of each note. (Of course in the mathematical definition, your post-it notes can be of any shape but of size no more than $\delta$) One thing you will notice if you do this, is that when you use smaller post it notes (make $\delta$ smaller) your approximation increases in accuracy (perhaps at the cost of the work of covering the object in more and more notes), and if you use small enough notes you will cover the object nearly perfectly. Hence:

\begin{definition}
Let $S$ be a subset of a metric space $X$. We define the $k$-dimensional Hausdorff measure of $S$ by:
\begin{align*}
\mathcal{H}^k(S) := \lim_{\delta \to 0} \mathcal{H}^k_\delta (S)
\end{align*}
\end{definition}

\begin{example}
When $k = 0$, we have that the $0$ dimensional Hausdorff measure of a set $S$, $\mathcal{H}^0(S)$ defined above is nothing more than the number of points in the set. If there is an infinite number of points, $\mathcal{H}^0(S) = \infty$.
\end{example}

In a sense, $\delta$ is the sort of resolution at which we are looking at the set to measure. Imagine we wish to measure an infinite spiral $S$. We know intuitively that the spiral should have infinite length, so it should be that $\mathcal{H}^1(S) = \infty$; but imagine we try to measure it by covering it with sets of some diameter $\varepsilon > 0$; we would necessarily cover up too much of the tail and get a finite number for $\mathcal{H}^1(S)$. Hence the reason why we need to look at the limiting value as $\delta \to 0$. 

Thus far we've talked about a notion of dimension $k$, and we know how to (at least implicitly) calculate the $k$-dimensional Hausdorff measure for a given set. But this gives rise to the question; which $k$ should I use to measure a given set I'm interested in? Will the answer change if I choose ``incorrectly"? This leads us to the derived notion of Hausdorff dimension.

\begin{definition}
Let $X$ be a metric space and $S \subseteq X$. Define:

\begin{align*}
\mathcal{H}\text{-dim}(S) := \sup \left\{ k \geq 0 : \mathcal{H}^k(S) < \infty \right\}
\end{align*}

Or equivalently

\begin{align*}
\mathcal{H}\text{-dim}(S) :=  \inf \left\{ k \geq 0 : \mathcal{H}^k(S) = \infty \right\}
\end{align*}
\end{definition}

The above is always defined for any set $S$. What this tells us is that there is that given a set $S$, there is a unique value of $k$ that is a sort of Goldilocks number. Suppose we're trying to measure the area of a piece of paper. If we use a $k = 1$ dimensional measure, then we are trying to essentially cover the paper with 1 dimensional sets - i.e. infinitely thin lines. Since we will need an infinite number of these lines to cover the page, we'll get a 1-dimensional measure of $\infty$. Suppose that we try to measure the piece of paper using a $k = 3$ dimensional measure, covering it with something like tennis balls or cubes. No matter what we choose, the piece of paper will only occupy an infinitely thin slice of the 3d object - hence its $k=3$ measure will be 0. In this case, the Goldilocks value of $k$ to use will be 2 - we can again use something like sticky notes to cover the piece of paper and figure out a reasonable idea of what its measure is. 

More precisely, there is some value of $k$ where $\mathcal{H}^k(S)$ flips from being $\infty$ to 0, measuring with sets of too small dimension to too big of dimension. This value of $k$ where it flips is what we call the dimension of $S$.

\begin{remark}
Note that at $k = \mathcal{H}$-dim$(S)$ the set $S$ may still have measure of $0$ or $\infty$. Imagine measuring the empty set, or perhaps all of $\mathbb{R}^2$ for examples.
\end{remark}

\begin{example}
Show the cantor set has Hausdorff dimension $\frac{\log 2}{ \log 3}$
\end{example}

\subsection{Rectifiable Sets}

Consider $\mathbb{R}^n$ as a space equipped with the standard euclidean metric and topology inherited from it.

\begin{definition}
A Borel subset $S$ of $\mathbb{R}^n$ is called $k$-rectifiable if $S$ has Hausdorff dimension $k$ and there exists a countable collection of continuously differentiable maps:

\begin{align*}
f_i: \mathbb{R}^k \to \mathbb{R}^n
\end{align*}

such that:

\begin{align*}
\mathcal{H}^k\left( S \setminus \bigcup_{i=1}^\infty f_i (\mathbb{R}^k) \right) = 0
\end{align*}
\end{definition}

That is to say, a Borel subset in $\mathbb{R}^n$ is $k$-rectifiable if it is approximately pieces of $\mathbb{R}^k$ (approximately in the sense of up to a set of $\mathcal{H}^k$ measure 0), deformed in a continuously differentiable manner.

\begin{remark}
Some authors prefer to equivalently define rectifiable sets using Lipschitz maps instead of continuously differentiable ones.
\end{remark}

Perhaps more intuitively, rectifiable sets may be thought of as generalizations of $k$-dimensional $C^1$-submanifolds. In the sense that they are countable unions of pieces of $k$-dimensional $C^1$-submanifolds (possibly with a measure 0 junk set) or of possessing approximate tangent spaces $\mathcal{H}^k$ almost everywhere.

We now seek to define these so-called approximate tangent spaces in terms of a procedure that ``blows-up" or ``zooms-in" at a point.

\begin{definition}
Let $S \subseteq \mathbb{R}^n$. Then a subspace $T$ of $\mathbb{R}^n$ is the approximate tangent space to $S$ at the point $p \in \mathbb{R}^n$ if for all compactly supported $f \in C_c(\mathbb{R}^n)$:

\begin{align*}
\lim_{\lambda \to 0^+} \int_{\eta_{p,\lambda}(S) } f(y) d\mathcal{H}^k(y) = \int_T f(y)d\mathcal{H}^k(y)
\end{align*}

Where $\eta_{p,\lambda}(y) := \frac{y-p}{\lambda}$.
\end{definition}

The above may look daunting, it's actually quite simple and very informative of what is going on. The map $\eta_{p,\lambda}(y)$ for $0 < \lambda < 1$ acts as a blow up by a factor of $\lambda$ for the set $S$ about the point $p$. The integrals against test functions is nothing more than the standard notion of weak convergence. Hence we say that $T$ is the tangent plane of $S$ at $p$ if we have weak convergence of the identity $1$ over the blow up of $S$ at $p$ to the identity over $T$ as $\lambda \to 0^+$. Even more simply, when we zoom in on the point $p$, our set $S$ approximately looks like $T$.


\subsection{$k$-Vectors and $k$-Forms}

%Todo: erase and use intro to manifolds, based on permutations
%tangent spaces -> k vectors spanning tan planes -> need areas so forms implies integrals

The goal of this section is to extend basic undergraduate multivariable calculus to a more general setting.

Typically we are taught to represent a vector $v$ anchored at a point $p$ in $\mathbb{R}^2$ by a column of numbers indicating its components:

\begin{align*}
v &= \begin{bmatrix}
v^1\\
v^2
\end{bmatrix}
\end{align*}

These components indicate how many units to move in the $x$ and $y$ direction from the point $p$ to draw an arrow that graphically represents $v$. We take the convention here of writing the components of vectors with superscripts.\\

Imagine that we have a function $f$ that models the trajectory of a particle through $\mathbb{R}^2$. Then a line can be drawn connecting any two points that $f$ passes through. If we take both points the line joins and let them approach a common point $p$ and the line stabilizes to some limiting position then we have determined a tangent to the curve at the point $p$ which will in a sense just touch the function there, and very importantly provide a linear approximation to the function around that point $p$. There are many possible vectors along this tangent, each with a different length and possibly opposite orientation, whose span yields the full tangent passing through the point. 

A similar construction can be made to a surface in $\mathbb{R}^3$ by looking at $3$ points on a surface that define a plane. This tangent plane is spanned by any two linearly independent vectors living in that space.

What we would like to do is to find a characterization of these concepts 

\subsubsection{Tangent Vectors}

\subsubsection{Vector Fields}

\subsubsection{Dual Spaces and Permutations}

\subsubsection{Multilinear Functions}

\subsubsection{Tensor Products}

\subsubsection{Wedge Products}

\subsubsection{Differential Forms}

\subsection{Currents}

%todo: fix definitions of rectifiable current and integer current

\begin{definition}
Let $M$ be a manifold. Then a $k$-dimensional current is any element of $(\omega_c^k(M))^*$, the dual space to the set of $k$-forms defined over $M$ with compact support.
\end{definition}

If $\Sigma$ is a compact submanifold of $M$ then we define a natural current associated with $\Sigma$ by $[\Sigma](\omega) := \int_\Sigma \omega$.

\begin{definition}
Let $T$ be a $k$-current on a Riemannian manifold $M$. Then we define the mass of $T$ by:

\begin{align*}
\mathcal{M}(T) := \sup \left\{ |\langle T,\omega \rangle| : \omega \in \Omega_c^k(M), |w|\leq 1 \right\}
\end{align*}
\end{definition}

\begin{theorem}
If $\Sigma$ is a compact submanifold of $M$ then $\mathcal{M}([\Sigma])$ is the area of $\Sigma$.
\end{theorem}

\begin{definition}
Let $T$ be a $k$-current on $M$. Then we define the $(k-1)$-current $\partial T$ called the boundary of $T$ by:

\begin{align*}
\langle \partial T, \eta \rangle := \langle T, d\eta \rangle
\end{align*}

Where by Stoke's theorem we have $\partial [\Sigma] = [\partial \Sigma]$ if $\Sigma$ is a compact submanifold. In other words:

\begin{align*}
\int_\Sigma d\eta = \int_{\partial \Sigma}\eta
\end{align*}
\end{definition}

\begin{definition}
Let $T$ be a $k-current$. Then $T$ is called $k$-rectifiable if there exists $\nu$ and $\tau$ such that:

\begin{align*}
\langle T,\omega \rangle &= \int_S \nu(x) \langle \omega(x), \tau(x) \rangle d\mathcal{H}^k(x)
\end{align*}

$\nu$ is called the multiplicity of $T$.

\end{definition}

\begin{definition}
A $k$-rectifiable current is called an integral current if $T$ has integer multiplicity almost everywhere.
\end{definition}

\subsection{The Flat Norm}

If $T$ is a integral $k$-current we can always write it as:

\begin{align*}
T &= R + \partial S
\end{align*}

Where $R$ is an integral $k$-current and $S$ is an integral $(k+1)$-current. 

\begin{definition}
We define the flat norm $\mathbb{F}(T)$ by:

\begin{align*}
\mathbb{F} := \inf \{\mathcal{M}(R) + \mathcal{M}(S)\}
\end{align*}

For all $R,S$ as above.
\end{definition}


\section{L1TV Computes the Flat Norm}

(Curtis)

\section{Answers and Questions}

(Kevin)

\section{Computing the Flat Norm}

(Blake and Bala?)

\end{document}
